{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# HCML project\n\n## Finetuning VGG16","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/rcmalli/keras-vggface.git\n!pip install keras_applications","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:59:23.848909Z","iopub.execute_input":"2023-06-30T07:59:23.849307Z","iopub.status.idle":"2023-06-30T07:59:41.669136Z","shell.execute_reply.started":"2023-06-30T07:59:23.849278Z","shell.execute_reply":"2023-06-30T07:59:41.667976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\nfrom keras.models import Model, Sequential\nfrom keras.applications.vgg16 import VGG16\nfrom keras_vggface.vggface import VGGFace\n\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\n\nimport numpy as np\nimport os\nimport cv2\nfrom more_itertools import chunked\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:59:41.671972Z","iopub.execute_input":"2023-06-30T07:59:41.672536Z","iopub.status.idle":"2023-06-30T07:59:41.679812Z","shell.execute_reply.started":"2023-06-30T07:59:41.672499Z","shell.execute_reply":"2023-06-30T07:59:41.678331Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"SEED = 78\nBATCH_SIZE = 64\nIMG_SIZE = 256 ","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:59:41.681592Z","iopub.execute_input":"2023-06-30T07:59:41.681980Z","iopub.status.idle":"2023-06-30T07:59:41.690584Z","shell.execute_reply.started":"2023-06-30T07:59:41.681948Z","shell.execute_reply":"2023-06-30T07:59:41.689461Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prepare_data(ds, data_aug):\n    normalization = layers.Rescaling(1./255)\n    ds = ds.map(lambda x, y: (normalization(x), y))\n\n    if data_aug:\n        brightness = layers.RandomBrightness((-0.3, 0.3), value_range=(0., 1.), seed=SEED)\n        rotation = layers.RandomRotation(0.2, seed=SEED)\n\n        rotated_ds = ds.map(lambda x, y: (rotation(x), y))\n        brightness_ds = ds.map(lambda x, y: (brightness(x), y))\n        rotated_brightness_ds = ds.map(lambda x, y: (brightness(rotation(x)), y))\n                \n        ds = tf.data.Dataset.sample_from_datasets(\n                [ds, rotated_ds, brightness_ds, rotated_brightness_ds]\n        )\n\n    return ds.batch(BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:59:41.693650Z","iopub.execute_input":"2023-06-30T07:59:41.694390Z","iopub.status.idle":"2023-06-30T07:59:41.703262Z","shell.execute_reply.started":"2023-06-30T07:59:41.694359Z","shell.execute_reply":"2023-06-30T07:59:41.702333Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/heysem-sorted-data/sorted_data/train\",\n    validation_split=0.3,\n    label_mode='binary',\n    subset=\"training\",\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=None,\n#     batch_size=BATCH_SIZE\n)\n\nval_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/heysem-sorted-data/sorted_data/train\",\n    validation_split=0.3,\n    label_mode='binary',\n    subset=\"validation\",\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=None,\n#     batch_size=BATCH_SIZE\n)\n\ntrain_ds = prepare_data(train_ds, data_aug=False)\nval_ds = prepare_data(val_ds, data_aug=False)\n\n#NOTE: the output here is incorrect and there is no verbose option...","metadata":{"execution":{"iopub.status.busy":"2023-06-30T07:59:41.704550Z","iopub.execute_input":"2023-06-30T07:59:41.705274Z","iopub.status.idle":"2023-06-30T07:59:48.987476Z","shell.execute_reply.started":"2023-06-30T07:59:41.705224Z","shell.execute_reply":"2023-06-30T07:59:48.986560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg= VGGFace(model='vgg16', include_top=False, pooling='avg', input_shape=(256, 256, 3))\n\n# Freeze the layers except the last 5\nfor layer in vgg.layers[:-5]:\n    layer.trainable = False\n# # Check the trainable status of the individual layers\n# for layer in vgg.layers:\n#     print(layer, layer.trainable)\n\n# Model\nmodel = keras.Sequential() # Add the VGG16 convolutional base model\nmodel.add(vgg)\n \n# Add new layers\nmodel.add(keras.layers.Dense(64, activation='relu'))\nmodel.add(keras.layers.BatchNormalization())\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:03:24.502809Z","iopub.execute_input":"2023-06-30T08:03:24.503200Z","iopub.status.idle":"2023-06-30T08:03:24.833171Z","shell.execute_reply.started":"2023-06-30T08:03:24.503169Z","shell.execute_reply":"2023-06-30T08:03:24.832199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early = EarlyStopping(monitor='val_binary_accuracy', \n                      patience=10, \n                      verbose=1, \n                      mode='auto')\n\nhist = model.fit(x=train_ds,\n                 validation_data=val_ds,\n                 epochs=40,\n                 callbacks=[early]\n                )","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:03:26.452231Z","iopub.execute_input":"2023-06-30T08:03:26.452599Z","iopub.status.idle":"2023-06-30T08:21:32.957160Z","shell.execute_reply.started":"2023-06-30T08:03:26.452571Z","shell.execute_reply":"2023-06-30T08:21:32.956097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save('/kaggle/working/vggface_v3_0.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:21:37.878172Z","iopub.execute_input":"2023-06-30T08:21:37.878553Z","iopub.status.idle":"2023-06-30T08:21:38.088573Z","shell.execute_reply.started":"2023-06-30T08:21:37.878523Z","shell.execute_reply":"2023-06-30T08:21:38.087556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model on balanced data","metadata":{}},{"cell_type":"code","source":"balanced_train_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/balanced-data/balanced_data/train\",\n    validation_split=0.3,\n    label_mode='binary',\n    subset=\"training\",\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=None,\n)\n\nbalanced_val_ds = tf.keras.utils.image_dataset_from_directory(\n    \"/kaggle/input/balanced-data/balanced_data/train\",\n    validation_split=0.3,\n    label_mode='binary',\n    subset=\"validation\",\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=None,\n)\n\nbalanced_train_ds = prepare_data(balanced_train_ds, data_aug=False)\nbalanced_val_ds = prepare_data(balanced_val_ds, data_aug=False)\n\n#NOTE: the output here is incorrect and there is no verbose option...","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:32:43.548331Z","iopub.execute_input":"2023-06-30T08:32:43.548849Z","iopub.status.idle":"2023-06-30T08:32:43.766665Z","shell.execute_reply.started":"2023-06-30T08:32:43.548808Z","shell.execute_reply":"2023-06-30T08:32:43.765510Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bal_vgg = VGGFace(model='vgg16', include_top=False, pooling='avg', input_shape=(256, 256, 3))\n\n# Freeze the layers except the last 5\nfor layer in bal_vgg.layers[:-5]:\n    layer.trainable = False\n# # Check the trainable status of the individual layers\n# for layer in vgg.layers:\n#     print(layer, layer.trainable)\n\n# Model\nbalanced_model = keras.Sequential() # Add the VGG16 convolutional base model\nbalanced_model.add(bal_vgg)\n \n# Add new layers\nbalanced_model.add(keras.layers.Dense(64, activation='relu'))\nbalanced_model.add(keras.layers.BatchNormalization())\nbalanced_model.add(keras.layers.Dense(1, activation='sigmoid'))\n\nbalanced_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['binary_accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:32:46.275461Z","iopub.execute_input":"2023-06-30T08:32:46.275856Z","iopub.status.idle":"2023-06-30T08:32:46.619135Z","shell.execute_reply.started":"2023-06-30T08:32:46.275825Z","shell.execute_reply":"2023-06-30T08:32:46.618143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early = EarlyStopping(monitor='val_binary_accuracy', \n                      patience=10, \n                      verbose=1, \n                      mode='auto')\n\nbalanced_hist = balanced_model.fit(x=balanced_train_ds,\n                 validation_data=balanced_val_ds,\n                 epochs=40,\n                 callbacks=[early]\n                )","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:32:50.621328Z","iopub.execute_input":"2023-06-30T08:32:50.622040Z","iopub.status.idle":"2023-06-30T08:34:21.776061Z","shell.execute_reply.started":"2023-06-30T08:32:50.622007Z","shell.execute_reply":"2023-06-30T08:34:21.774987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"balanced_model.save('/kaggle/working/vggface_balanced_v1_0.h5')","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:34:21.777892Z","iopub.execute_input":"2023-06-30T08:34:21.778883Z","iopub.status.idle":"2023-06-30T08:34:22.005532Z","shell.execute_reply.started":"2023-06-30T08:34:21.778847Z","shell.execute_reply":"2023-06-30T08:34:22.004471Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Assessing performance","metadata":{}},{"cell_type":"code","source":"model = keras.models.load_model(\"/kaggle/working/vggface_balanced_v1_0.h5\")\n\nhist = balanced_hist","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:39:37.925392Z","iopub.execute_input":"2023-06-30T08:39:37.925805Z","iopub.status.idle":"2023-06-30T08:39:38.403720Z","shell.execute_reply.started":"2023-06-30T08:39:37.925767Z","shell.execute_reply":"2023-06-30T08:39:38.402716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.plot(hist.history[\"binary_accuracy\"])\nplt.plot(hist.history['val_binary_accuracy'])\n# plt.plot(hist.history['loss'])\n# plt.plot(hist.history['val_loss'])\nplt.title(\"Model accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.legend([\"Accuracy\",\"Validation Accuracy\",\"Loss\",\"Validation Loss\"])\nplt.show()\n\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:39:38.968323Z","iopub.execute_input":"2023-06-30T08:39:38.968682Z","iopub.status.idle":"2023-06-30T08:39:39.568683Z","shell.execute_reply.started":"2023-06-30T08:39:38.968653Z","shell.execute_reply":"2023-06-30T08:39:39.567678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"meta_data = pd.read_csv(\"/kaggle/input/meta-data/eth_gender_anno_all.xlsx - eth_gender_trait_annotations_de.csv\")\n# use same encoding \nmeta_data['Gender'] = meta_data[\"Gender\"].replace(2, 0)\n\n# clear difference in occurences in data\nmeta_data[\"Ethnicity\"].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:39:48.872096Z","iopub.execute_input":"2023-06-30T08:39:48.872468Z","iopub.status.idle":"2023-06-30T08:39:48.897332Z","shell.execute_reply.started":"2023-06-30T08:39:48.872439Z","shell.execute_reply":"2023-06-30T08:39:48.896240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_ds = tf.keras.utils.image_dataset_from_directory(\n#     \"/kaggle/input/sorted-heysem-dataset/sorted_data/test\",\n#     label_mode='binary',\n#     seed=SEED,\n#     image_size=(IMG_SIZE, IMG_SIZE),\n#     batch_size=None,\n# #     batch_size=BATCH_SIZE\n# )\n# test_ds = prepare_data(test_ds, training=False)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:39:57.378326Z","iopub.execute_input":"2023-06-30T08:39:57.378698Z","iopub.status.idle":"2023-06-30T08:39:57.383789Z","shell.execute_reply.started":"2023-06-30T08:39:57.378669Z","shell.execute_reply":"2023-06-30T08:39:57.382601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def load_and_preprocess_image(image_path):\n    # NOTE: ensure this matches the original model\n    img = cv2.imread(image_path)\n    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n    img = img / 255.0  # Normalize pixel values\n    return img","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:39:57.579712Z","iopub.execute_input":"2023-06-30T08:39:57.580401Z","iopub.status.idle":"2023-06-30T08:39:57.586124Z","shell.execute_reply.started":"2023-06-30T08:39:57.580368Z","shell.execute_reply":"2023-06-30T08:39:57.585117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# src = \"/kaggle/input/heysem-sorted-data/sorted_data/test/\"\nsrc = \"/kaggle/input/balanced-data/balanced_data/test/\"\nwomen_paths = [os.path.join(src, \"women/\", file) for file in os.listdir(src + \"women/\")]# if file.endswith('.JPG')]\nmen_paths = [os.path.join(src, \"men/\", file) for file in os.listdir(src + \"men/\")]# if file.endswith('.JPG')]\n\nwomen_data = [load_and_preprocess_image(path) for path in women_paths]\nmen_data = [load_and_preprocess_image(path) for path in men_paths]\n\nwomen_batches = chunked(women_data, BATCH_SIZE)\nmen_batches = chunked(men_data, BATCH_SIZE)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:15.131439Z","iopub.execute_input":"2023-06-30T08:40:15.131823Z","iopub.status.idle":"2023-06-30T08:40:15.627104Z","shell.execute_reply.started":"2023-06-30T08:40:15.131793Z","shell.execute_reply":"2023-06-30T08:40:15.626012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_predictions = []\nfor batch in women_batches:\n    dataset = tf.data.Dataset.from_tensor_slices(batch).batch(64)\n\n    # because sigmoid output of one node\n    preds = model.predict(dataset) < 0.5 \n    w_predictions.append(preds.astype(int))","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:16.781178Z","iopub.execute_input":"2023-06-30T08:40:16.782404Z","iopub.status.idle":"2023-06-30T08:40:34.032278Z","shell.execute_reply.started":"2023-06-30T08:40:16.782353Z","shell.execute_reply":"2023-06-30T08:40:34.031161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"m_predictions = []\nfor batch in men_batches:\n    dataset = tf.data.Dataset.from_tensor_slices(batch).batch(64)\n\n    # because sigmoid output of one node\n    preds = model.predict(dataset) < 0.5 \n    m_predictions.append(preds.astype(int))","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:34.034169Z","iopub.execute_input":"2023-06-30T08:40:34.034568Z","iopub.status.idle":"2023-06-30T08:40:43.320679Z","shell.execute_reply.started":"2023-06-30T08:40:34.034531Z","shell.execute_reply":"2023-06-30T08:40:43.319739Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"w_preds = [i[0] for pred in w_predictions for i in pred]\nm_preds = [i[0] for pred in m_predictions for i in pred]","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:43.322382Z","iopub.execute_input":"2023-06-30T08:40:43.322834Z","iopub.status.idle":"2023-06-30T08:40:43.328511Z","shell.execute_reply.started":"2023-06-30T08:40:43.322799Z","shell.execute_reply":"2023-06-30T08:40:43.327267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results_dict = {}\n\nfor pred, path in zip(w_preds, women_paths):\n    trim_path = path[-23:-4]\n    row = meta_data.loc[meta_data[\"VideoName\"] == trim_path]\n    \n    ethnicity, gender = tuple(row[[\"Ethnicity\", \"Gender\"]].values[0])\n\n    results_dict[trim_path] = (pred, ethnicity, gender)\n    \nfor pred, path in zip(m_preds, men_paths):\n    trim_path = path[-23:-4]\n    row = meta_data.loc[meta_data[\"VideoName\"] == trim_path]\n    \n    ethnicity, gender = tuple(row[[\"Ethnicity\", \"Gender\"]].values[0])\n\n    results_dict[trim_path] = (pred, ethnicity, gender)","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:43.331515Z","iopub.execute_input":"2023-06-30T08:40:43.333798Z","iopub.status.idle":"2023-06-30T08:40:43.725212Z","shell.execute_reply.started":"2023-06-30T08:40:43.333724Z","shell.execute_reply":"2023-06-30T08:40:43.724262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame.from_dict(results_dict).T\ndf.columns = [\"gender_prediction\", \"ethnicity\", \"gender_true\"]\nprint(df.shape)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:40:43.726844Z","iopub.execute_input":"2023-06-30T08:40:43.727222Z","iopub.status.idle":"2023-06-30T08:40:43.749620Z","shell.execute_reply.started":"2023-06-30T08:40:43.727188Z","shell.execute_reply":"2023-06-30T08:40:43.748423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# index to ethnicity according to orignal dataset\ni2e = {1: \"Asian\", 2:\"Caucasian\", 3:\"African-American\"}\n\n# print(\"WOMEN\")\nacc = round(sum(df['gender_prediction'] == df['gender_true'])/len(df), 3)\ncm = confusion_matrix(df[\"gender_true\"], df[\"gender_prediction\"])\ndisp = ConfusionMatrixDisplay(cm)\ndisp.plot()\nplt.title(f\"Overal, acc={acc}\")\nplt.savefig(\"/kaggle/working/balanced_plots/overall.png\")\nplt.show()\n\nfor e in i2e.keys():\n    e_df = df.loc[df['ethnicity']==e]\n    \n    acc = round(sum(e_df['gender_prediction'] == e_df['gender_true'])/len(e_df), 3)\n\n    cm = confusion_matrix(e_df[\"gender_true\"], e_df[\"gender_prediction\"])\n    disp = ConfusionMatrixDisplay(cm)\n    disp.plot()\n    plt.title(f\"{i2e[e]}, acc={acc}\")\n    plt.savefig(f\"/kaggle/working/balanced_plots/{i2e[e]}.png\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:44:54.224112Z","iopub.execute_input":"2023-06-30T08:44:54.224519Z","iopub.status.idle":"2023-06-30T08:44:55.981978Z","shell.execute_reply.started":"2023-06-30T08:44:54.224490Z","shell.execute_reply":"2023-06-30T08:44:55.980279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir /kaggle/working/balanced_plots","metadata":{"execution":{"iopub.status.busy":"2023-06-30T08:45:19.202907Z","iopub.execute_input":"2023-06-30T08:45:19.203282Z","iopub.status.idle":"2023-06-30T08:45:20.385304Z","shell.execute_reply.started":"2023-06-30T08:45:19.203253Z","shell.execute_reply":"2023-06-30T08:45:20.383936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"src = \"/kaggle/input/sorted-heysem-dataset/sorted_data/train/\"\ntrain_women_paths = [os.path.join(src, \"woman/\", file) for file in os.listdir(src + \"woman/\")]# if file.endswith('.JPG')]\ntrain_men_paths = [os.path.join(src, \"man/\", file) for file in os.listdir(src + \"man/\")]# if file.endswith('.JPG')]\n\nresults_dict = {}\n\n# for pred, path in zip(w_preds, train_women_paths):\nfor path in train_women_paths:\n\n    trim_path = path[-23:-4]\n    row = meta_data.loc[meta_data[\"VideoName\"] == trim_path]\n    \n    ethnicity, gender = tuple(row[[\"Ethnicity\", \"Gender\"]].values[0])\n\n    results_dict[trim_path] = (ethnicity, gender)\n    \n# for pred, path in zip(m_preds, train_men_paths):\nfor path in train_men_paths:\n    trim_path = path[-23:-4]\n    row = meta_data.loc[meta_data[\"VideoName\"] == trim_path]\n    \n    ethnicity, gender = tuple(row[[\"Ethnicity\", \"Gender\"]].values[0])\n\n    results_dict[trim_path] = (ethnicity, gender)\n    \n\ndf = pd.DataFrame.from_dict(results_dict).T\ndf.columns = [\"ethnicity\", \"gender_true\"]\n# print(df.shape)\n# df.head()\n\ndf['ethnicity'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-06-29T13:52:44.589836Z","iopub.execute_input":"2023-06-29T13:52:44.590885Z","iopub.status.idle":"2023-06-29T13:53:05.534652Z","shell.execute_reply.started":"2023-06-29T13:52:44.590848Z","shell.execute_reply":"2023-06-29T13:53:05.533724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"i2e = {1: \"Asian\", 2:\"Caucasian\", 3:\"African-American\"}","metadata":{"execution":{"iopub.status.busy":"2023-06-29T13:53:05.536131Z","iopub.execute_input":"2023-06-29T13:53:05.536786Z","iopub.status.idle":"2023-06-29T13:53:05.542509Z","shell.execute_reply.started":"2023-06-29T13:53:05.536743Z","shell.execute_reply":"2023-06-29T13:53:05.541533Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Archive\nstoring some stuff that we might still use","metadata":{}},{"cell_type":"code","source":"# TO PLOT IMAGE\n# count = 0 \n# # for batch in all_ds[2]:\n# for batch in b:\n\n#     count += 1\n#     img = batch[0].numpy()#.astype(\"uint8\")\n#     plt.imshow(img)\n#     plt.show()\n    \n#     if count > 10: break\n        \n\n        # output_path = '/kaggle/working/output'\n\n# checkpoint = ModelCheckpoint(filepath=output_path, \n#                              monitor='val_acc', \n#                              verbose=1, \n#                              save_best_only=True, \n#                              save_weights_only=False, \n#                              mode='auto', \n#                              period=1)\n\n\n# test_ds = tf.keras.utils.image_dataset_from_directory(\n#     \"/kaggle/input/sorted-heysem-dataset/sorted_data/test\",\n#     label_mode='binary',\n#     seed=SEED,\n#     image_size=(IMG_SIZE, IMG_SIZE),\n#     batch_size=None,\n# #     batch_size=BATCH_SIZE\n# )\n# test_ds = prepare_data(test_ds, training=False)\n\n# preds = model.predict(test_ds) >= 0.5 # because sigmoid output of one node\n# preds = preds.astype(int)\n    ","metadata":{},"execution_count":null,"outputs":[]}]}